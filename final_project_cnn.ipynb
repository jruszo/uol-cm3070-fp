{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.350860Z",
     "start_time": "2024-03-08T23:08:25.535600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Initialize dataset\n",
    "\n",
    "We initialize the dataset object, define required transformations and create the training and evaluation data loaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbbe4c209a848466"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "use_mps = True and torch.backends.mps.is_available()\n",
    "test_batch_size = 1000\n",
    "epochs = 14\n",
    "lr = 1.0\n",
    "gamma = 0.7\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "save_model = False\n",
    "\n",
    "if use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.392952Z",
     "start_time": "2024-03-08T23:08:27.388732Z"
    }
   },
   "id": "b0ff50d1064ad157",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.444619Z",
     "start_time": "2024-03-08T23:08:27.391740Z"
    }
   },
   "id": "d9874ea412a357ce",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 28, 28])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[0][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.470284Z",
     "start_time": "2024-03-08T23:08:27.449618Z"
    }
   },
   "id": "969388a6a8648987",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Define the model creation function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e3bcd630bcd7de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConstructNet(nn.Module):\n",
    "    def __init__(self, DNA, loss_fn, input_size, output_size):\n",
    "        #print(DNA)\n",
    "        super(ConstructNet, self).__init__()\n",
    "        self.DNA = DNA\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = []\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Append first layer\n",
    "        self.layers.append(nn.Conv2d(1, 2 * self.DNA[0][1], kernel_size=3, stride=1, padding=1))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(1, len(self.DNA)):\n",
    "            #print(self.DNA[i])\n",
    "            if self.DNA[i][0] == \"C\":\n",
    "                self.layers.append(nn.Conv2d(2 * self.DNA[i-1][1], 2 * self.DNA[i][1], kernel_size=3, stride=1, padding=1))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            if self.DNA[i][0] == \"D\":\n",
    "                # The input size is the output of the last layer\n",
    "                tmp_input_size = self.last_layer_output_size()\n",
    "                self.layers.append(nn.Linear(tmp_input_size, self.DNA[i][1]))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            if self.DNA[i][0] == \"R\":\n",
    "                self.layers.append(nn.Dropout(self.DNA[i][1]))\n",
    "\n",
    "        # Append the output layer \n",
    "        self.layers.append(nn.Flatten())\n",
    "        \n",
    "        cnn_output = self.cnn_output_size()\n",
    "        self.layers.append(nn.Linear(cnn_output[1], self.output_size*10))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(self.output_size*10, self.output_size))\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers, nn.Softmax(dim=1))\n",
    "    def cnn_output_size(self):\n",
    "        input = torch.ones(64, 1, self.input_size,self.input_size)\n",
    "        #print(input.shape)\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        #print(self.layers)\n",
    "        return input.shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        return self.net(x)\n",
    "\n",
    "    '''\n",
    "    Based on the layers created, find the output size of the last dense layer\n",
    "    '''\n",
    "\n",
    "    def last_layer_output_size(self):\n",
    "        for layer in self.layers[::-1]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                return layer.out_features\n",
    "\n",
    "    def train_net(self, device, train_loader, optimizer, epoch, log_interval, print_stats):\n",
    "        self.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(data)\n",
    "            #print(output)\n",
    "            loss = self.loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0 and print_stats:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    def test(self, device, test_loader, print_stats=False):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = self.forward(data)\n",
    "                #print(f\"output: {output}, target: {target}\")\n",
    "                test_loss += self.loss_fn.forward(output, target).item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        if print_stats:\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * accuracy))\n",
    "        return test_loss, accuracy\n",
    "\n",
    "    def count_parameters(self):\n",
    "        # https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/7\n",
    "        return sum(p.numel() for p in self.net.parameters() if p.requires_grad)        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.482015Z",
     "start_time": "2024-03-08T23:08:27.477886Z"
    }
   },
   "id": "3a383f68d06c15f",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Add GA functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "524e0621b44ba832"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "class GA():\n",
    "    def __init__(self, generation_length, population_size, initial_size, initial_depth, \n",
    "                 lossFn, input_size, output_size, device, train_dataloader, test_dataloader, optimizerFn, epoch):\n",
    "        self.generation_length = generation_length\n",
    "        self.population_size = population_size\n",
    "        self.initial = True # No previous individuals can be used for mutations\n",
    "        self.initial_size = initial_size\n",
    "        self.initial_depth = initial_depth\n",
    "        \n",
    "        self.lossFn = lossFn\n",
    "        self.optimizerFn = optimizerFn\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.epoch = epoch\n",
    "        self.population = []\n",
    "        \n",
    "        #self.generate_population()\n",
    "    \n",
    "    def auto_evolve(self):\n",
    "        for i in range(self.generation_length):\n",
    "            #print(f\"Generation #{i}\")\n",
    "            self.generate_population()\n",
    "            self.train_population()\n",
    "            self.evaluate_population()\n",
    "            self.save_population(gen_nm=i)\n",
    "            self.calculate_fitness()\n",
    "            self.print_statistics(gen_nm=i, raw=True)\n",
    "    \n",
    "    def save_population(self, gen_nm):\n",
    "        os.mkdir(f\"models/gen_{gen_nm}\")\n",
    "        for index,individual in enumerate(self.population):\n",
    "            acc = individual['accuracy']\n",
    "            params = individual['parameters']\n",
    "            torch.save(individual['model'], f\"models/gen_{gen_nm}/ind_{index}_{params}_{acc}.pt\")\n",
    "    def print_statistics(self, gen_nm, raw=False):\n",
    "        fitness_list = []\n",
    "        parameters_list = []\n",
    "        accuracy_list = []\n",
    "        for item in self.population:\n",
    "            fitness_list.append(item['fitness'])\n",
    "            parameters_list.append(item['parameters'])\n",
    "            accuracy_list.append(item['accuracy'])\n",
    "        if raw:\n",
    "            print(f\"{gen_nm}, {np.min(fitness_list)}, {np.min(parameters_list)}, {np.min(accuracy_list)}, {np.mean(fitness_list)}, {np.mean(parameters_list)}, {np.mean(accuracy_list)}, {np.max(fitness_list)}, {np.max(parameters_list)}, {np.max(accuracy_list)}\")\n",
    "        else:\n",
    "            print(f\"Average fitness: {np.mean(fitness_list)}, Average parameters: {np.mean(parameters_list)}, Average accuracy: {np.mean(accuracy_list)}, Min fitness: {np.min(fitness_list)}, Min parameters: {np.min(parameters_list)}, Min accuracy: {np.min(accuracy_list)}, Max fitness: {np.max(fitness_list)}, Max parameters: {np.max(parameters_list)}, Max accuracy: {np.max(accuracy_list)}\")\n",
    "        \n",
    "        \n",
    "    def generate_population(self):\n",
    "        tmp_list = []\n",
    "        for index,item in enumerate(self.population):\n",
    "            tmp_list.append({\n",
    "                'genome': item['genome'],\n",
    "                'model': item['model'],\n",
    "            })\n",
    "        self.population = tmp_list\n",
    "        del tmp_list\n",
    "        \n",
    "        if self.initial:\n",
    "            for i in range(self.population_size):\n",
    "                genome = self.generate_individual()\n",
    "                self.population.append({'genome': genome})\n",
    "        else:\n",
    "            # Delete 33% of the population\n",
    "            del self.population[:int(np.floor(self.population_size/3))]\n",
    "            # Generate new individuals to have the same population size\n",
    "            for i in range(self.population_size-len(self.population)):\n",
    "                genome = self.generate_individual()\n",
    "                self.population.append({'genome': genome})\n",
    "        # Create models for each individual        \n",
    "        for index,individual in enumerate(self.population):\n",
    "            model = ConstructNet(individual['genome'], self.lossFn, self.input_size, self.output_size)\n",
    "            model.to(device)\n",
    "            individual['model'] = model\n",
    "            self.population[index] = individual\n",
    "        self.initial = False\n",
    "            \n",
    "    def generate_individual(self):\n",
    "        genome = []\n",
    "        if self.initial:\n",
    "            for chromosome in range(random.randint(1, self.initial_depth)):\n",
    "                layer_size = random.randint(1,self.initial_size)\n",
    "                #layer_size = random.randint(np.floor(self.initial_size/2), np.ceil(self.initial_size*2))\n",
    "                genome.append([\"C\",layer_size])\n",
    "        else:\n",
    "            # Get the DNA of one of the best 5 individuals, that are at the end of the list\n",
    "            random_index = random.randint(len(self.population)-5, len(self.population)-1)\n",
    "            #print(random_index)\n",
    "            original_genome = self.population[random_index]['genome'].copy()\n",
    "            genome = self.mutate_genome(original_genome)\n",
    "        #print(genome)\n",
    "        return genome\n",
    "    \n",
    "    def mutate_genome(self, genome):\n",
    "        # example genome: [[\"D\",1024],[\"D\",512],[\"D\",128]]\n",
    "        # possible mutations: Add new larger layer, add new smaller layer, change layer size, remove layer\n",
    "        # Added +1 to new layer size to prevent 0 sized layers\n",
    "        # 1. Add new larger layer\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            new_layer_size = genome[layer_index][1] + 1\n",
    "            genome.insert(layer_index, [\"C\",new_layer_size])\n",
    "        # 2. Add new smaller layer\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            new_layer_size = max(genome[layer_index][1] - 1,1)\n",
    "            genome.insert(layer_index, [\"C\",new_layer_size])\n",
    "        # 3. Change layer size by growing or shrinking\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            direction = random.choice([-1,1])\n",
    "            new_layer_size = genome[layer_index][1] + direction\n",
    "            if len(genome) == 1 and new_layer_size < 1:\n",
    "                pass\n",
    "            elif len(genome) > 1 and new_layer_size < 1:\n",
    "                del genome[layer_index]\n",
    "            else:\n",
    "                genome[layer_index][1] = new_layer_size\n",
    "        # 4. Remove layer\n",
    "        if random.random() < 0.15 and len(genome) > 1:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            del genome[layer_index]\n",
    "        return genome\n",
    "    \n",
    "    def print_population(self, top_n=False):\n",
    "        if not top_n:\n",
    "            top_n = len(self.population)\n",
    "        for index,individual in enumerate(self.population[-top_n:]):\n",
    "            print(f\"Individual #{index}: {individual}\")\n",
    "            \n",
    "    def train_population(self):\n",
    "        #print(self.population)\n",
    "        for index,individual in enumerate(self.population):\n",
    "            #print(f\"Training individual #{index}\")\n",
    "            #print(individual['model'])\n",
    "            if self.optimizerFn == \"Adam\":\n",
    "                optimizer = torch.optim.Adam(individual['model'].parameters(), lr=0.01)\n",
    "            individual['model'].train_net(self.device,self.train_dataloader, optimizer,self.epoch, 100, False)\n",
    "    \n",
    "    def evaluate_population(self):\n",
    "        for index,individual in enumerate(self.population):\n",
    "            #print(f\"Evaluating individual #{index}\")\n",
    "            loss, accuracy = individual['model'].test(self.device, self.test_dataloader)\n",
    "            individual['id'] = index\n",
    "            individual['loss'] = loss\n",
    "            individual['accuracy'] = accuracy\n",
    "            individual['parameters'] = individual['model'].count_parameters()\n",
    "            self.population[index] = individual\n",
    "        \n",
    "    def calculate_fitness(self):\n",
    "        # Create copy of list using only a few columns\n",
    "        tmp_list = []\n",
    "        for index,item in enumerate(self.population):\n",
    "            tmp_list.append({\n",
    "                'id' : item['id'],\n",
    "                'accuracy': item['accuracy'],\n",
    "                'parameters': item['parameters'],\n",
    "                'fitness': 0\n",
    "            })\n",
    "        accuracy_list = sorted(tmp_list, key=lambda d: (d['accuracy'],d['parameters']))\n",
    "        # Adding more emphasis on accuracy\n",
    "        for index,item in enumerate(accuracy_list):\n",
    "            accuracy_list[index]['fitness'] += index * 10 * item['accuracy']\n",
    "        # Reverse sort, smaller is better\n",
    "        parameter_size_list = sorted(accuracy_list, key=lambda d: d['parameters'], reverse=True)\n",
    "        for index,item in enumerate(parameter_size_list):\n",
    "            pass\n",
    "            #parameter_size_list[index]['fitness'] += index\n",
    "        sorted_by_fitness = sorted(parameter_size_list, key=lambda d: d['fitness'])\n",
    "        for fitness in sorted_by_fitness:\n",
    "            for index,individual in enumerate(self.population):\n",
    "                if individual['id'] == fitness['id']:\n",
    "                    individual['fitness'] = fitness['fitness']\n",
    "            self.population[index] = individual\n",
    "        # Sort population\n",
    "        self.population = sorted(self.population, key=lambda d: d['fitness'])\n",
    "        #print(self.population)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:08:27.547151Z",
     "start_time": "2024-03-08T23:08:27.492707Z"
    }
   },
   "id": "9c9a8edb2270b95",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.0, 315404, 0.0958, 40.1999, 1670996.7333333334, 0.19628000000000004, 214.832, 3150132, 0.7408\n",
      "1, 0.0, 315348, 0.0958, 52.38473333333334, 1287945.5333333334, 0.25392333333333333, 233.856, 3150132, 0.8064\n",
      "2, 0.0, 315348, 0.0959, 62.6222, 766954.9333333333, 0.30787, 235.27700000000002, 3150132, 0.8113\n",
      "3, 0.0, 314750, 0.0915, 77.23963333333333, 482818.93333333335, 0.3909633333333333, 233.04399999999998, 1730508, 0.8036\n",
      "4, 0.0, 157930, 0.098, 80.7477, 404007.93333333335, 0.41276999999999997, 263.465, 944808, 0.9085\n",
      "5, 0.0, 157930, 0.098, 108.21976666666667, 346302.5333333333, 0.6385966666666665, 270.715, 942630, 0.9335\n",
      "6, 0.0, 314750, 0.101, 99.57373333333334, 372377.2, 0.5808666666666666, 235.799, 942630, 0.8131\n",
      "7, 0.0, 314750, 0.1011, 97.29889999999999, 346274.86666666664, 0.56108, 241.106, 471900, 0.8314\n",
      "8, 0.0, 314750, 0.098, 99.11146666666667, 351473.2, 0.5580166666666667, 272.281, 471900, 0.9389\n",
      "9, 0.0, 314750, 0.1135, 105.94506666666666, 346232.6, 0.5970133333333335, 265.495, 471900, 0.9155\n",
      "10, 0.0, 314990, 0.0958, 85.99206666666667, 440392.5333333333, 0.4490233333333334, 259.78200000000004, 472364, 0.8958\n",
      "11, 0.0, 314990, 0.098, 97.78006666666667, 450858.26666666666, 0.5215299999999999, 250.589, 472358, 0.8641\n",
      "12, 0.0, 314990, 0.0982, 97.73943333333334, 456081.86666666664, 0.5752766666666667, 268.91700000000003, 472102, 0.9273\n",
      "13, 0.0, 314990, 0.0958, 93.12346666666666, 456006.2, 0.5089566666666666, 263.407, 472028, 0.9083\n",
      "14, 0.0, 314750, 0.1135, 102.88776666666668, 445578.3333333333, 0.59235, 261.522, 472284, 0.9018\n",
      "15, 0.0, 314990, 0.098, 101.78686666666667, 461242.0, 0.5922733333333334, 266.8, 471900, 0.92\n",
      "16, 0.0, 314750, 0.098, 106.50403333333334, 429980.8, 0.5957866666666667, 267.09000000000003, 472358, 0.921\n",
      "17, 0.0, 314750, 0.0892, 105.1838, 429912.5333333333, 0.6282733333333334, 269.84499999999997, 471920, 0.9305\n",
      "18, 0.0, 314750, 0.0982, 98.90713333333333, 435169.93333333335, 0.5599566666666667, 267.844, 472028, 0.9236\n",
      "19, 0.0, 314750, 0.0981, 100.67016666666669, 435198.0, 0.5738233333333335, 239.192, 472340, 0.8248\n",
      "20, 0.0, 314750, 0.1009, 94.65063333333335, 419538.3333333333, 0.51435, 256.389, 472650, 0.8841\n",
      "21, 0.0, 314990, 0.0974, 85.979, 456103.2, 0.45415, 263.204, 472448, 0.9076\n",
      "22, 0.0, 158136, 0.098, 91.0811, 435140.6, 0.5020100000000001, 234.523, 472028, 0.8087\n",
      "23, 0.0, 158136, 0.1009, 101.37633333333333, 435224.73333333334, 0.5670166666666667, 267.93100000000004, 472342, 0.9239\n",
      "24, 0.0, 158136, 0.098, 95.33903333333335, 435331.8, 0.5048666666666667, 256.012, 472866, 0.8828\n",
      "25, 0.0, 158136, 0.098, 100.67056666666664, 403875.8, 0.5361766666666666, 262.566, 472284, 0.9054\n",
      "26, 0.0, 157930, 0.0958, 105.55236666666666, 330665.6, 0.63927, 249.574, 472778, 0.8606\n",
      "27, 0.0, 157930, 0.098, 115.44110000000002, 257378.33333333334, 0.70403, 267.815, 472028, 0.9235\n",
      "28, 0.0, 157930, 0.0982, 110.79053333333334, 231207.73333333334, 0.628, 268.047, 471900, 0.9243\n",
      "29, 0.0, 157930, 0.1009, 118.07686666666667, 178886.6, 0.7197466666666668, 268.308, 471772, 0.9252\n",
      "30, 0.0, 157930, 0.0958, 121.96146666666667, 163179.2, 0.7479266666666667, 270.338, 314990, 0.9322\n",
      "31, 0.0, 157930, 0.101, 126.28999999999999, 163174.13333333333, 0.7952533333333333, 270.193, 314990, 0.9317\n",
      "32, 0.0, 157930, 0.5766, 121.60240000000002, 157934.4, 0.78408, 264.915, 158024, 0.9135\n",
      "33, 0.0, 157930, 0.5212, 125.0552, 157938.86666666667, 0.8104566666666668, 265.611, 158006, 0.9159\n",
      "34, 0.0, 157930, 0.1135, 116.40256666666666, 157940.13333333333, 0.7406100000000001, 271.063, 157968, 0.9347\n",
      "35, 0.0, 157930, 0.1135, 118.63903333333333, 157958.2, 0.72765, 269.497, 158080, 0.9293\n",
      "36, 0.0, 314750, 0.2596, 115.74733333333333, 314755.6, 0.7039533333333332, 272.07800000000003, 314806, 0.9382\n",
      "37, 0.0, 314750, 0.1009, 111.33436666666667, 455921.13333333336, 0.6282366666666667, 268.569, 471828, 0.9261\n",
      "38, 0.0, 314750, 0.0958, 110.6341, 419356.2, 0.6173699999999998, 271.643, 471920, 0.9367\n",
      "39, 0.0, 157930, 0.0958, 105.51756666666667, 325233.4, 0.5996266666666668, 265.64, 472028, 0.916\n",
      "40, 0.0, 157930, 0.098, 120.33016666666667, 231159.73333333334, 0.7152966666666667, 267.728, 472028, 0.9232\n",
      "41, 0.0, 157930, 0.0958, 111.52, 330504.4666666667, 0.6435833333333334, 262.073, 472448, 0.9037\n",
      "42, 0.0, 157930, 0.1135, 121.16946666666666, 189299.66666666666, 0.7355499999999998, 267.61199999999997, 471570, 0.9228\n",
      "43, 0.0, 157930, 0.0982, 124.2065, 241574.13333333333, 0.7726633333333334, 267.177, 314898, 0.9213\n",
      "44, 0.0, 157930, 0.101, 118.74309999999998, 246812.66666666666, 0.7242833333333334, 266.655, 314898, 0.9195\n",
      "45, 0.0, 157930, 0.0958, 111.3847, 215490.53333333333, 0.64451, 263.87100000000004, 314898, 0.9099\n",
      "46, 0.0, 314750, 0.098, 112.0851666666667, 314765.4666666667, 0.6407166666666667, 266.829, 314898, 0.9201\n",
      "47, 0.0, 157930, 0.098, 119.1828666666667, 272944.3333333333, 0.7002433333333332, 266.858, 314898, 0.9202\n",
      "48, 0.0, 157930, 0.5823, 124.48956666666666, 173627.06666666668, 0.7996333333333332, 266.162, 314750, 0.9178\n",
      "49, 0.0, 157930, 0.0958, 115.91126666666666, 163193.8, 0.6773299999999999, 261.841, 314750, 0.9029\n",
      "50, 0.0, 157930, 0.2889, 121.88166666666666, 178908.66666666666, 0.7558466666666666, 262.62399999999997, 314750, 0.9056\n",
      "51, 0.0, 157930, 0.1028, 118.62280000000001, 189318.93333333332, 0.7175966666666667, 269.584, 314806, 0.9296\n",
      "52, 0.0, 157930, 0.0958, 111.30506666666666, 293869.8, 0.6592166666666667, 267.554, 314990, 0.9226\n",
      "53, 0.0, 157930, 0.3023, 120.74669999999999, 241587.93333333332, 0.7434866666666667, 272.107, 314806, 0.9383\n",
      "54, 0.0, 157930, 0.3738, 113.03399999999998, 262497.8, 0.7101266666666669, 273.499, 314898, 0.9431\n",
      "55, 0.0, 157930, 0.098, 108.07879999999999, 278263.26666666666, 0.6302133333333333, 259.057, 471772, 0.8933\n",
      "56, 0.0, 157930, 0.1009, 106.02170000000001, 262565.13333333336, 0.60084, 268.395, 471772, 0.9255\n",
      "57, 0.0, 157930, 0.3619, 116.58483333333334, 257314.26666666666, 0.71287, 270.889, 471772, 0.9341\n",
      "58, 0.0, 314750, 0.2994, 116.5797, 319996.4666666667, 0.7087600000000001, 268.569, 471772, 0.9261\n",
      "59, 0.0, 314750, 0.3981, 119.18276666666667, 314764.3333333333, 0.7326599999999999, 270.077, 314844, 0.9313\n",
      "60, 0.0, 157930, 0.113, 108.55143333333334, 382735.73333333334, 0.65306, 262.62399999999997, 471682, 0.9056\n",
      "61, 0.0, 157930, 0.098, 103.06263333333335, 382765.0, 0.5495533333333333, 267.438, 471772, 0.9222\n",
      "62, 0.0, 158024, 0.5001, 113.57723333333335, 304344.8, 0.70444, 268.772, 315082, 0.9268\n",
      "63, 0.0, 158024, 0.098, 96.23886666666667, 440240.4666666667, 0.5335833333333332, 246.5, 471900, 0.85\n",
      "64, 0.0, 158024, 0.101, 109.7472, 309577.8, 0.6660433333333333, 267.815, 314990, 0.9235\n",
      "65, 0.0, 158024, 0.0958, 108.08949999999999, 309576.06666666665, 0.6253799999999999, 264.364, 314990, 0.9116\n",
      "66, 0.0, 157930, 0.1028, 114.41073333333335, 194564.0, 0.7048833333333335, 268.192, 314806, 0.9248\n",
      "67, 0.0, 157930, 0.4128, 115.84846666666665, 189326.66666666666, 0.7224366666666666, 269.236, 314806, 0.9284\n",
      "68, 0.0, 314750, 0.2782, 118.22096666666667, 314788.93333333335, 0.7359300000000001, 267.38, 314990, 0.922\n",
      "69, 0.0, 157930, 0.1009, 116.8988, 304333.13333333336, 0.6888133333333334, 266.423, 315064, 0.9187\n",
      "70, 0.0, 157930, 0.1028, 110.44053333333332, 325270.93333333335, 0.6367733333333333, 265.81399999999996, 629138, 0.9166\n",
      "71, 0.0, 157930, 0.0982, 120.53116666666666, 325256.8, 0.7438, 272.59999999999997, 629138, 0.94\n",
      "72, 0.0, 157930, 0.3036, 117.46466666666667, 178864.93333333332, 0.7496833333333331, 263.90000000000003, 314750, 0.91\n",
      "73, 0.0, 157930, 0.1135, 115.78083333333332, 173634.6, 0.7161766666666668, 264.741, 314750, 0.9129\n",
      "74, 0.0, 157930, 0.098, 121.11819999999999, 157944.46666666667, 0.7508966666666667, 265.553, 158024, 0.9157\n",
      "75, 0.0, 157930, 0.098, 117.93196666666668, 210217.06666666668, 0.7134566666666668, 268.569, 314898, 0.9261\n",
      "76, 0.0, 157930, 0.5365, 114.25133333333333, 205002.26666666666, 0.7336600000000001, 262.276, 314898, 0.9044\n",
      "77, 0.0, 157930, 0.465, 121.42443333333334, 189309.0, 0.7690166666666668, 269.033, 314898, 0.9277\n",
      "78, 0.0, 157930, 0.4596, 120.42670000000001, 194551.13333333333, 0.77771, 263.581, 314990, 0.9089\n",
      "79, 0.0, 157930, 0.4268, 122.25386666666667, 173642.73333333334, 0.7657900000000001, 267.554, 314806, 0.9226\n",
      "80, 0.0, 157930, 0.1028, 122.84009999999999, 184089.2, 0.7645933333333333, 272.02, 314956, 0.938\n",
      "81, 0.0, 157930, 0.1011, 118.76123333333334, 189316.6, 0.7157366666666666, 268.91700000000003, 314806, 0.9273\n",
      "82, 0.0, 157930, 0.1009, 111.74990000000001, 299089.6666666667, 0.6419266666666665, 267.06100000000004, 314990, 0.9209\n",
      "83, 0.0, 157930, 0.098, 111.07773333333333, 367059.6666666667, 0.6625033333333333, 266.916, 471900, 0.9204\n",
      "84, 0.0, 157930, 0.098, 114.20836666666666, 236373.53333333333, 0.6561133333333333, 267.525, 471900, 0.9225\n",
      "85, 0.0, 157930, 0.1135, 118.06216666666666, 225907.2, 0.7131833333333334, 271.382, 471570, 0.9358\n",
      "86, 0.0, 157930, 0.0982, 110.63476666666665, 304318.8, 0.6659700000000001, 261.174, 314990, 0.9006\n",
      "87, 0.0, 157930, 0.098, 114.60093333333334, 340954.2, 0.6804433333333334, 270.454, 471828, 0.9326\n",
      "88, 0.0, 157930, 0.1035, 111.63030000000002, 283427.86666666664, 0.6579533333333334, 267.873, 471772, 0.9237\n",
      "89, 0.0, 157930, 0.098, 122.1488, 184085.2, 0.7531166666666667, 267.177, 471772, 0.9213\n",
      "90, 0.0, 157930, 0.1135, 121.8169, 210254.46666666667, 0.7382900000000002, 267.264, 471772, 0.9216\n",
      "91, 0.0, 157930, 0.1135, 117.7808, 241581.6, 0.7074366666666666, 272.803, 314898, 0.9407\n",
      "92, 0.0, 157930, 0.1135, 117.50453333333334, 225891.0, 0.7259966666666667, 271.15000000000003, 314806, 0.935\n",
      "93, 0.0, 157930, 0.101, 114.53926666666668, 314805.06666666665, 0.6765933333333333, 268.453, 472028, 0.9257\n",
      "94, 0.0, 157930, 0.1028, 110.24873333333335, 215453.4, 0.6283966666666667, 266.597, 471570, 0.9193\n",
      "95, 0.0, 157930, 0.5362, 119.61916666666667, 199763.2, 0.7582233333333334, 265.64, 471570, 0.916\n",
      "96, 0.0, 157930, 0.098, 115.9215333333333, 283413.06666666665, 0.69741, 269.961, 472028, 0.9309\n",
      "97, 0.0, 157930, 0.0958, 114.1460333333333, 314777.0, 0.6654266666666666, 270.135, 472028, 0.9315\n",
      "98, 0.0, 157930, 0.098, 108.21453333333334, 309615.3333333333, 0.6043866666666666, 270.715, 471772, 0.9335\n",
      "99, 0.0, 157930, 0.1135, 114.87323333333332, 304346.0, 0.68143, 265.234, 471570, 0.9146\n"
     ]
    }
   ],
   "source": [
    "generation = GA(generation_length=100,population_size=30,initial_size=20,initial_depth=5, lossFn=nn.CrossEntropyLoss(), input_size=28, output_size=10, device=device, train_dataloader=train_loader, test_dataloader=test_loader, optimizerFn=\"Adam\", epoch=15  )\n",
    "generation.auto_evolve()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T04:20:28.868206Z",
     "start_time": "2024-03-08T23:08:27.496160Z"
    }
   },
   "id": "1c00315d63f2f84c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T04:20:28.884582Z",
     "start_time": "2024-03-09T04:20:28.861286Z"
    }
   },
   "id": "5148bb0371fd1a1d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T04:20:28.884800Z",
     "start_time": "2024-03-09T04:20:28.862661Z"
    }
   },
   "id": "a3f9a3cab08cc452",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
