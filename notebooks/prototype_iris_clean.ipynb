{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:16.794331Z",
     "start_time": "2024-01-07T12:51:15.428350Z"
    }
   },
   "id": "5972842f85194c6a",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create dataset object for dataloader for Iris dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ffb5a27c4e158b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Iris_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.x = torch.tensor(data, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:16.800648Z",
     "start_time": "2024-01-07T12:51:16.795713Z"
    }
   },
   "id": "5730d79f43b30860",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/IRIS.csv')\n",
    "df_norm = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "target = labelencoder.fit_transform(data['species'])\n",
    "target = pd.DataFrame(target)\n",
    "target.rename(columns={0: 'species'}, inplace=True)\n",
    "df = pd.concat([df_norm, target], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:17.389290Z",
     "start_time": "2024-01-07T12:51:16.798959Z"
    }
   },
   "id": "78d619b39fa4526e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConstructNet(nn.Module):\n",
    "    def __init__(self, DNA, loss_fn, input_size, output_size, outputlayer):\n",
    "        #print(DNA)\n",
    "        super(ConstructNet, self).__init__()\n",
    "        self.DNA = DNA\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = []\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Append first layer\n",
    "        self.layers.append(nn.Linear(self.input_size, self.DNA[0][1]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(1, len(self.DNA)):\n",
    "            #print(self.DNA[i])\n",
    "            if self.DNA[i][0] == \"D\":\n",
    "                # The input size is the output of the last layer\n",
    "                tmp_input_size = self.last_layer_output_size()\n",
    "                self.layers.append(nn.Linear(tmp_input_size, self.DNA[i][1]))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            if self.DNA[i][0] == \"R\":\n",
    "                self.layers.append(nn.Dropout(self.DNA[i][1]))\n",
    "\n",
    "        # Append the output layer        \n",
    "        self.layers.append(nn.Linear(self.layers[-2].out_features, self.output_size))\n",
    "        #if self.output_size > 1: \n",
    "        #    self.layers.append(nn.Softmax(dim=1))\n",
    "        #else:\n",
    "        #    self.layers.append(nn.Sigmoid())\n",
    "        self.layers.append(outputlayer)\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    '''\n",
    "    Based on the layers created, find the output size of the last dense layer\n",
    "    '''\n",
    "\n",
    "    def last_layer_output_size(self):\n",
    "        for layer in self.layers[::-1]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                return layer.out_features\n",
    "\n",
    "    def train_net(self, device, train_loader, optimizer, epoch, log_interval, print_stats):\n",
    "        self.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(data)\n",
    "            loss = self.loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0 and print_stats:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    def test(self, device, test_loader, print_stats=False):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = self.forward(data)\n",
    "                #print(f\"output: {output}, target: {target}\")\n",
    "                test_loss += self.loss_fn.forward(output, target).item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        if print_stats:\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * accuracy))\n",
    "        return test_loss, accuracy\n",
    "\n",
    "    def count_parameters(self):\n",
    "        # https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/7\n",
    "        return sum(p.numel() for p in self.net.parameters() if p.requires_grad)        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:17.398103Z",
     "start_time": "2024-01-07T12:51:17.394452Z"
    }
   },
   "id": "3eeaf5bb8278af77",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class GA():\n",
    "    def __init__(self, generation_length, population_size, initial_size, initial_depth, \n",
    "                 lossFn, input_size, output_size, outputLayer, device, train_dataloader, test_dataloader, optimizerFn, epoch):\n",
    "        self.generation_length = generation_length\n",
    "        self.population_size = population_size\n",
    "        self.initial = True # No previous individuals can be used for mutations\n",
    "        self.initial_size = initial_size\n",
    "        self.initial_depth = initial_depth\n",
    "        \n",
    "        self.lossFn = lossFn\n",
    "        self.optimizerFn = optimizerFn\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.outputLayer = outputLayer\n",
    "        \n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.epoch = epoch\n",
    "        self.population = []\n",
    "        \n",
    "        #self.generate_population()\n",
    "    \n",
    "    def auto_evolve(self):\n",
    "        for i in range(self.generation_length):\n",
    "            #print(f\"Generation #{i}\")\n",
    "            self.generate_population()\n",
    "            self.train_population()\n",
    "            self.evaluate_population()\n",
    "            self.calculate_fitness()\n",
    "            self.print_statistics(raw=True)\n",
    "    \n",
    "    def print_statistics(self, raw=False):\n",
    "        fitness_list = []\n",
    "        parameters_list = []\n",
    "        accuracy_list = []\n",
    "        for item in self.population:\n",
    "            fitness_list.append(item['fitness'])\n",
    "            parameters_list.append(item['parameters'])\n",
    "            accuracy_list.append(item['accuracy'])\n",
    "        if raw:\n",
    "            print(f\"{np.mean(fitness_list)}, {np.mean(parameters_list)}, {np.mean(accuracy_list)}, {np.min(fitness_list)}, {np.min(parameters_list)}, {np.min(accuracy_list)}, {np.max(fitness_list)}, {np.max(parameters_list)}, {np.max(accuracy_list)}\")\n",
    "        else:\n",
    "            print(f\"Average fitness: {np.mean(fitness_list)}, Average parameters: {np.mean(parameters_list)}, Average accuracy: {np.mean(accuracy_list)}, Min fitness: {np.min(fitness_list)}, Min parameters: {np.min(parameters_list)}, Min accuracy: {np.min(accuracy_list)}, Max fitness: {np.max(fitness_list)}, Max parameters: {np.max(parameters_list)}, Max accuracy: {np.max(accuracy_list)}\")\n",
    "        \n",
    "        \n",
    "    def generate_population(self):\n",
    "        tmp_list = []\n",
    "        for index,item in enumerate(self.population):\n",
    "            tmp_list.append({\n",
    "                'genome': item['genome'],\n",
    "                'model': item['model'],\n",
    "            })\n",
    "        self.population = tmp_list\n",
    "        del tmp_list\n",
    "        \n",
    "        if self.initial:\n",
    "            for i in range(self.population_size):\n",
    "                genome = self.generate_individual()\n",
    "                self.population.append({'genome': genome})\n",
    "        else:\n",
    "            # Delete 33% of the population\n",
    "            del self.population[:int(np.floor(self.population_size/3))]\n",
    "            # Generate new individuals to have the same population size\n",
    "            for i in range(self.population_size-len(self.population)):\n",
    "                genome = self.generate_individual()\n",
    "                self.population.append({'genome': genome})\n",
    "        # Create models for each individual        \n",
    "        for index,individual in enumerate(self.population):\n",
    "            model = ConstructNet(individual['genome'], self.lossFn, self.input_size, self.output_size, self.outputLayer)\n",
    "            individual['model'] = model\n",
    "            self.population[index] = individual\n",
    "        self.initial = False\n",
    "            \n",
    "    def generate_individual(self):\n",
    "        genome = []\n",
    "        if self.initial:\n",
    "            for chromosome in range(random.randint(np.floor(self.initial_depth/2), np.ceil(self.initial_depth*2))):\n",
    "                layer_size = random.randint(np.floor(self.initial_size/2), np.ceil(self.initial_size*2))\n",
    "                genome.append([\"D\",layer_size])\n",
    "        else:\n",
    "            # Get the DNA of one of the best 5 individuals, that are at the end of the list\n",
    "            random_index = random.randint(len(self.population)-10, len(self.population)-1)\n",
    "            #print(random_index)\n",
    "            original_genome = self.population[random_index]['genome'].copy()\n",
    "            genome = self.mutate_genome(original_genome)\n",
    "        #print(genome)\n",
    "        return genome\n",
    "    \n",
    "    def mutate_genome(self, genome):\n",
    "        # example genome: [[\"D\",1024],[\"D\",512],[\"D\",128]]\n",
    "        # possible mutations: Add new larger layer, add new smaller layer, change layer size, remove layer\n",
    "        # Added +1 to new layer size to prevent 0 sized layers\n",
    "        # 1. Add new larger layer\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            new_layer_size = np.ceil(genome[layer_index][1] * (1 + random.random())).astype(int) + 1\n",
    "            genome.insert(layer_index, [\"D\",new_layer_size])\n",
    "        # 2. Add new smaller layer\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            new_layer_size = np.floor(genome[layer_index][1] * (1 - random.random())).astype(int) + 1\n",
    "            genome.insert(layer_index, [\"D\",new_layer_size])\n",
    "        # 3. Change layer size\n",
    "        if random.random() < 0.15:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            new_layer_size = np.round(genome[layer_index][1] * random.random()).astype(int) + 1\n",
    "            genome[layer_index][1] = new_layer_size\n",
    "        # 4. Remove layer\n",
    "        if random.random() < 0.15 and len(genome) > 1:\n",
    "            # select random layer\n",
    "            layer_index = random.randint(0, len(genome)-1)\n",
    "            del genome[layer_index]\n",
    "        return genome\n",
    "    \n",
    "    def print_population(self, top_n=False):\n",
    "        if not top_n:\n",
    "            top_n = len(self.population)\n",
    "        for index,individual in enumerate(self.population[-top_n:]):\n",
    "            print(f\"Individual #{index}: {individual}\")\n",
    "            \n",
    "    def train_population(self):\n",
    "        #print(self.population)\n",
    "        for index,individual in enumerate(self.population):\n",
    "            #print(f\"Training individual #{index}\")\n",
    "            if self.optimizerFn == \"Adam\":\n",
    "                optimizer = torch.optim.Adam(individual['model'].parameters(), lr=0.01)\n",
    "            individual['model'].train_net(self.device,self.train_dataloader, optimizer,self.epoch, 100, False)\n",
    "    \n",
    "    def evaluate_population(self):\n",
    "        for index,individual in enumerate(self.population):\n",
    "            #print(f\"Evaluating individual #{index}\")\n",
    "            loss, accuracy = individual['model'].test(self.device, self.test_dataloader)\n",
    "            individual['id'] = index\n",
    "            individual['loss'] = loss\n",
    "            individual['accuracy'] = accuracy\n",
    "            individual['parameters'] = individual['model'].count_parameters()\n",
    "            self.population[index] = individual\n",
    "        \n",
    "    def calculate_fitness(self):\n",
    "        # Create copy of list using only a few columns\n",
    "        tmp_list = []\n",
    "        for index,item in enumerate(self.population):\n",
    "            tmp_list.append({\n",
    "                'id' : item['id'],\n",
    "                'accuracy': item['accuracy'],\n",
    "                'parameters': item['parameters'],\n",
    "                'fitness': 0\n",
    "            })\n",
    "        accuracy_list = sorted(tmp_list, key=lambda d: (d['accuracy'],d['parameters']))\n",
    "        # Adding more emphasis on accuracy\n",
    "        for index,item in enumerate(accuracy_list):\n",
    "            accuracy_list[index]['fitness'] += index * 10 * item['accuracy']\n",
    "        # Reverse sort, smaller is better\n",
    "        parameter_size_list = sorted(accuracy_list, key=lambda d: d['parameters'], reverse=True)\n",
    "        for index,item in enumerate(parameter_size_list):\n",
    "            pass\n",
    "            #parameter_size_list[index]['fitness'] += index\n",
    "        sorted_by_fitness = sorted(parameter_size_list, key=lambda d: d['fitness'])\n",
    "        for fitness in sorted_by_fitness:\n",
    "            for index,individual in enumerate(self.population):\n",
    "                if individual['id'] == fitness['id']:\n",
    "                    individual['fitness'] = fitness['fitness']\n",
    "            self.population[index] = individual\n",
    "        # Sort population\n",
    "        self.population = sorted(self.population, key=lambda d: d['fitness'])\n",
    "        #print(self.population)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:17.409973Z",
     "start_time": "2024-01-07T12:51:17.396990Z"
    }
   },
   "id": "ef81c4cc57f2801a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "use_mps = False and torch.backends.mps.is_available()\n",
    "use_cuda = True and torch.cuda.is_available()\n",
    "test_batch_size = 16\n",
    "epochs = 30\n",
    "lr = 0.02\n",
    "gamma = 0.7\n",
    "seed = 1\n",
    "log_interval = 100\n",
    "save_model = False\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:51:17.417834Z",
     "start_time": "2024-01-07T12:51:17.410908Z"
    }
   },
   "id": "c906071a984e5420",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508.63955555555555, 477008.9266666667, 0.5997777777777777, 0.0, 33199, 0.3333333333333333, 1400.6, 1147624, 0.94\n",
      "542.1475555555555, 295032.05333333334, 0.6525777777777777, 0.0, 1459, 0.3333333333333333, 1420.4666666666667, 1138476, 0.9533333333333334\n",
      "534.7422222222223, 222264.32666666666, 0.6526222222222221, 0.0, 1459, 0.3333333333333333, 1440.3333333333333, 888871, 0.9666666666666667\n",
      "535.0297777777778, 162208.14, 0.6548444444444445, 0.0, 163, 0.3333333333333333, 1430.3999999999999, 888871, 0.96\n",
      "524.9364444444445, 121194.71333333333, 0.6055111111111111, 0.0, 163, 0.3333333333333333, 1440.3333333333333, 888871, 0.9666666666666667\n",
      "564.9391111111111, 211946.14666666667, 0.6823999999999999, 0.0, 139, 0.3333333333333333, 1420.4666666666667, 1288166, 0.9533333333333334\n",
      "570.3902222222223, 154525.06, 0.7072, 0.0, 139, 0.3333333333333333, 1440.3333333333333, 1288166, 0.9666666666666667\n",
      "541.3853333333334, 776658.6266666667, 0.6250222222222223, 0.0, 123, 0.3333333333333333, 1430.3999999999999, 5062678, 0.96\n",
      "557.664, 185436.47333333333, 0.6567555555555555, 0.0, 139, 0.29333333333333333, 1450.2666666666667, 3091394, 0.9733333333333334\n",
      "574.8448888888888, 112434.32, 0.7168888888888888, 0.0, 139, 0.3333333333333333, 1440.3333333333333, 1737274, 0.9666666666666667\n",
      "579.1191111111111, 125103.26, 0.6975555555555556, 0.0, 323, 0.3333333333333333, 1440.3333333333333, 761793, 0.9666666666666667\n",
      "577.1084444444444, 136185.35333333333, 0.7107555555555556, 0.0, 323, 0.32, 1450.2666666666667, 761793, 0.9733333333333334\n",
      "550.2324444444445, 138671.34666666668, 0.6660888888888888, 0.0, 323, 0.3333333333333333, 1440.3333333333333, 755394, 0.9666666666666667\n",
      "555.308, 128724.98, 0.6843555555555556, 0.0, 9373, 0.3333333333333333, 1440.3333333333333, 626885, 0.9666666666666667\n",
      "510.47733333333326, 86130.82, 0.5884444444444443, 0.0, 19, 0.3333333333333333, 1420.4666666666667, 587932, 0.9533333333333334\n",
      "539.6128888888888, 108441.92, 0.6513333333333332, 0.0, 43, 0.3333333333333333, 1440.3333333333333, 453584, 0.9666666666666667\n",
      "543.6826666666665, 111109.28, 0.6712888888888888, 0.0, 2795, 0.3333333333333333, 1410.5333333333333, 570436, 0.9466666666666667\n",
      "546.5453333333332, 87727.24666666667, 0.6631555555555555, 0.0, 83, 0.3333333333333333, 1430.3999999999999, 570436, 0.96\n",
      "529.8604444444444, 78055.25333333333, 0.6018666666666667, 0.0, 163, 0.32666666666666666, 1440.3333333333333, 570436, 0.9666666666666667\n",
      "543.2088888888889, 81653.67333333334, 0.6427555555555555, 0.0, 19, 0.3333333333333333, 1450.2666666666667, 531600, 0.9733333333333334\n",
      "533.2955555555557, 56745.32666666667, 0.6646666666666666, 0.0, 339, 0.3333333333333333, 1430.3999999999999, 463808, 0.96\n",
      "538.7493333333333, 51787.06, 0.6555555555555557, 0.0, 339, 0.3333333333333333, 1440.3333333333333, 365078, 0.9666666666666667\n",
      "537.2608888888888, 44446.3, 0.6825333333333333, 0.0, 227, 0.3333333333333333, 1410.5333333333333, 365078, 0.9466666666666667\n",
      "510.0471111111111, 58080.386666666665, 0.618711111111111, 0.0, 253, 0.3333333333333333, 1410.5333333333333, 437906, 0.9466666666666667\n",
      "518.7946666666667, 46238.1, 0.6391111111111112, 0.0, 467, 0.3333333333333333, 1430.3999999999999, 221561, 0.96\n",
      "491.9093333333333, 55070.82, 0.5917777777777776, 0.0, 467, 0.3333333333333333, 1350.9333333333332, 362984, 0.9066666666666666\n",
      "471.66844444444445, 71147.55333333333, 0.5370666666666667, 0.0, 361, 0.29333333333333333, 1440.3333333333333, 1248806, 0.9666666666666667\n",
      "456.4168888888888, 61581.86666666667, 0.5225777777777777, 0.0, 27, 0.16, 1350.9333333333332, 1248806, 0.9066666666666666\n",
      "474.51466666666664, 50970.28, 0.5430222222222221, 0.0, 27, 0.25333333333333335, 1440.3333333333333, 1248806, 0.9666666666666667\n",
      "471.2297777777777, 43012.92, 0.5393333333333333, 0.0, 27, 0.3333333333333333, 1390.6666666666667, 1270698, 0.9333333333333333\n",
      "428.12533333333334, 36322.933333333334, 0.48373333333333335, 0.0, 27, 0.02666666666666667, 1430.3999999999999, 1267794, 0.96\n",
      "530.9204444444443, 65440.013333333336, 0.6155555555555554, 0.0, 51, 0.3333333333333333, 1430.3999999999999, 1267794, 0.96\n",
      "485.44399999999996, 39594.793333333335, 0.5761333333333333, 0.0, 179, 0.3333333333333333, 1231.7333333333333, 1267794, 0.8266666666666667\n",
      "430.4751111111111, 32794.44666666666, 0.48644444444444446, 0.0, 71, 0.07333333333333333, 1430.3999999999999, 1267794, 0.96\n",
      "399.6546666666666, 33468.16, 0.45351111111111114, 0.0, 75, 0.02666666666666667, 1341.0, 1267794, 0.9\n",
      "422.7088888888889, 436034.12666666665, 0.4823555555555555, 0.0, 591, 0.3333333333333333, 1211.8666666666668, 3958071, 0.8133333333333334\n",
      "479.4279999999999, 510127.19333333336, 0.5455111111111111, 0.0, 591, 0.3333333333333333, 1410.5333333333333, 3857499, 0.9466666666666667\n",
      "459.4422222222222, 370939.4733333333, 0.5290666666666667, 0.0, 591, 0.3333333333333333, 1341.0, 3857499, 0.9\n",
      "469.57111111111124, 547266.5666666667, 0.5375555555555555, 0.0, 655, 0.3333333333333333, 1440.3333333333333, 3660027, 0.9666666666666667\n",
      "491.09822222222226, 1140996.02, 0.5724444444444445, 0.0, 655, 0.3333333333333333, 1440.3333333333333, 32750104, 0.9666666666666667\n",
      "510.56533333333334, 100267.84666666666, 0.6072888888888889, 0.0, 655, 0.3333333333333333, 1410.5333333333333, 1953161, 0.9466666666666667\n",
      "525.9359999999999, 125878.32666666666, 0.6371111111111111, 0.0, 91, 0.3333333333333333, 1450.2666666666667, 6981116, 0.9733333333333334\n",
      "522.2408888888889, 35955.513333333336, 0.6545333333333334, 0.0, 621, 0.3333333333333333, 1440.3333333333333, 630285, 0.9666666666666667\n",
      "440.99022222222214, 8044.166666666667, 0.5028888888888889, 0.0, 199, 0.3333333333333333, 1430.3999999999999, 339182, 0.96\n",
      "476.8168888888889, 61313.58, 0.548711111111111, 0.0, 158, 0.3333333333333333, 1440.3333333333333, 502312, 0.9666666666666667\n",
      "517.3404444444445, 23614.24, 0.6198222222222222, 0.0, 158, 0.3333333333333333, 1390.6666666666667, 339182, 0.9333333333333333\n",
      "482.4724444444445, 17500.7, 0.5681333333333334, 0.0, 428, 0.31333333333333335, 1380.7333333333333, 230107, 0.9266666666666666\n",
      "465.5471111111111, 8512.806666666667, 0.5316000000000001, 0.0, 175, 0.17333333333333334, 1390.6666666666667, 159432, 0.9333333333333333\n",
      "406.1853333333334, 8624.86, 0.4617777777777777, 0.0, 151, 0.3333333333333333, 1341.0, 158976, 0.9\n",
      "372.52355555555556, 9324.893333333333, 0.4298222222222222, 0.0, 116, 0.32, 1291.3333333333335, 157286, 0.8666666666666667\n",
      "411.3075555555555, 24890.006666666668, 0.46599999999999997, 0.0, 95, 0.3333333333333333, 1400.6, 155596, 0.94\n",
      "461.6026666666666, 30871.853333333333, 0.5286222222222222, 0.0, 83, 0.3333333333333333, 1380.7333333333333, 199941, 0.9266666666666666\n",
      "485.3364444444444, 36168.87333333334, 0.5708888888888889, 0.0, 227, 0.3333333333333333, 1321.1333333333334, 289570, 0.8866666666666667\n",
      "450.0964444444444, 16514.113333333335, 0.5142666666666665, 0.0, 43, 0.32666666666666666, 1420.4666666666667, 277738, 0.9533333333333334\n",
      "414.80266666666665, 18031.306666666667, 0.474, 0.0, 43, 0.2866666666666667, 1162.2, 273114, 0.78\n",
      "381.5328888888889, 25904.693333333333, 0.4374666666666666, 0.0, 27, 0.18, 1271.4666666666667, 233286, 0.8533333333333334\n",
      "378.08, 32708.153333333332, 0.4351999999999999, 0.0, 19, 0.2866666666666667, 1201.9333333333334, 233286, 0.8066666666666666\n",
      "353.7608888888888, 32379.306666666667, 0.4122222222222222, 0.0, 19, 0.3333333333333333, 1420.4666666666667, 261048, 0.9533333333333334\n",
      "391.93377777777783, 32959.38, 0.4480888888888888, 0.0, 104, 0.32666666666666666, 1341.0, 261048, 0.9\n",
      "449.1, 32804.18, 0.5076, 0.0, 187, 0.02666666666666667, 1430.3999999999999, 261048, 0.96\n",
      "458.564, 25652.233333333334, 0.509511111111111, 0.0, 23, 0.3333333333333333, 1440.3333333333333, 261048, 0.9666666666666667\n",
      "492.19599999999997, 50760.46, 0.5641333333333333, 0.0, 19, 0.26, 1410.5333333333333, 279218, 0.9466666666666667\n",
      "524.747111111111, 20859.273333333334, 0.6500444444444445, 0.0, 27, 0.3333333333333333, 1420.4666666666667, 169490, 0.9533333333333334\n",
      "402.5808888888889, 4693.34, 0.45902222222222216, 0.0, 11, 0.3333333333333333, 1360.8666666666666, 51649, 0.9133333333333333\n",
      "302.9524444444445, 5542.066666666667, 0.37177777777777776, 0.0, 11, 0.2866666666666667, 1221.8, 256081, 0.82\n",
      "347.8782222222222, 10483.433333333332, 0.4044444444444444, 0.0, 11, 0.0, 1410.5333333333333, 573579, 0.9466666666666667\n",
      "441.24666666666667, 9625.433333333332, 0.49831111111111115, 0.0, 11, 0.26, 1390.6666666666667, 256081, 0.9333333333333333\n",
      "463.367111111111, 11544.526666666667, 0.5296444444444444, 0.0, 439, 0.3333333333333333, 1410.5333333333333, 256081, 0.9466666666666667\n",
      "473.1982222222222, 24906.84, 0.5416444444444445, 0.0, 439, 0.3333333333333333, 1430.3999999999999, 256081, 0.96\n",
      "477.7622222222222, 19257.726666666666, 0.5495555555555555, 0.0, 91, 0.3333333333333333, 1430.3999999999999, 256081, 0.96\n",
      "377.52933333333334, 21729.446666666667, 0.4342222222222222, 0.0, 18, 0.30666666666666664, 1311.2, 257445, 0.88\n",
      "374.0324444444444, 37930.926666666666, 0.43022222222222223, 0.0, 11, 0.3333333333333333, 1430.3999999999999, 257445, 0.96\n",
      "343.316, 26675.94, 0.4039111111111111, 0.0, 19, 0.29333333333333333, 1211.8666666666668, 256081, 0.8133333333333334\n",
      "389.0373333333333, 50541.166666666664, 0.44448888888888877, 0.0, 19, 0.14666666666666667, 1271.4666666666667, 280974, 0.8533333333333334\n",
      "381.61066666666665, 45522.12666666666, 0.4384444444444445, 0.0, 35, 0.32, 1221.8, 256907, 0.82\n",
      "326.14311111111107, 55907.64666666667, 0.3899555555555556, 0.0, 58, 0.31333333333333335, 1162.2, 347297, 0.78\n",
      "336.6013333333333, 63119.1, 0.3968, 0.0, 1270, 0.19333333333333333, 1390.6666666666667, 347297, 0.9333333333333333\n",
      "339.01644444444446, 46617.36666666667, 0.40044444444444444, 0.0, 28, 0.32, 1211.8666666666668, 347297, 0.8133333333333334\n",
      "301.75644444444447, 54245.77333333333, 0.37088888888888893, 0.0, 35, 0.3333333333333333, 1360.8666666666666, 347297, 0.9133333333333333\n",
      "361.9631111111112, 59301.39333333333, 0.4177333333333333, 0.0, 1270, 0.3333333333333333, 1440.3333333333333, 309732, 0.9666666666666667\n",
      "437.08088888888886, 49614.0, 0.4973333333333333, 0.0, 79, 0.3333333333333333, 1400.6, 245282, 0.94\n",
      "517.352, 42254.56, 0.5903999999999999, 0.0, 91, 0.3333333333333333, 1440.3333333333333, 239802, 0.9666666666666667\n",
      "520.048, 28595.426666666666, 0.6384, 0.0, 91, 0.3333333333333333, 1400.6, 314642, 0.94\n",
      "511.8999999999999, 35136.333333333336, 0.6272, 0.0, 91, 0.31333333333333335, 1440.3333333333333, 339975, 0.9666666666666667\n",
      "508.71466666666663, 33215.55333333334, 0.6170222222222221, 0.0, 99, 0.3333333333333333, 1440.3333333333333, 339975, 0.9666666666666667\n",
      "478.3093333333333, 15862.913333333334, 0.5525777777777777, 0.0, 35, 0.3333333333333333, 1410.5333333333333, 190362, 0.9466666666666667\n",
      "428.64355555555557, 46279.39333333333, 0.48457777777777766, 0.0, 55, 0.18666666666666668, 1440.3333333333333, 672337, 0.9666666666666667\n",
      "360.1351111111111, 64400.72, 0.41800000000000004, 0.0, 55, 0.3333333333333333, 1321.1333333333334, 456739, 0.8866666666666667\n",
      "365.56399999999996, 58759.113333333335, 0.4233777777777778, 0.0, 37, 0.3333333333333333, 1241.6666666666667, 418629, 0.8333333333333334\n",
      "359.85333333333335, 28269.46, 0.4186222222222222, 0.0, 19, 0.3333333333333333, 1341.0, 240384, 0.9\n",
      "368.66533333333336, 26714.413333333334, 0.42599999999999993, 0.0, 39, 0.2866666666666667, 1301.2666666666667, 240384, 0.8733333333333333\n",
      "365.96177777777774, 41723.833333333336, 0.42373333333333324, 0.0, 227, 0.29333333333333333, 1430.3999999999999, 525079, 0.96\n",
      "364.2853333333333, 36003.02, 0.4216, 0.0, 51, 0.32, 1420.4666666666667, 486443, 0.9533333333333334\n",
      "427.23466666666667, 58319.17333333333, 0.48528888888888894, 0.0, 147, 0.3333333333333333, 1420.4666666666667, 614191, 0.9533333333333334\n",
      "334.4871111111111, 33979.973333333335, 0.3961333333333333, 0.0, 32, 0.3333333333333333, 1370.8, 551281, 0.92\n",
      "322.296, 37064.973333333335, 0.38706666666666667, 0.0, 11, 0.3333333333333333, 1162.2, 551281, 0.78\n",
      "391.01111111111106, 51349.60666666667, 0.44608888888888903, 0.0, 11, 0.32666666666666666, 1321.1333333333334, 545785, 0.8866666666666667\n",
      "369.95199999999994, 58870.7, 0.4276, 0.0, 147, 0.3333333333333333, 1281.4, 545785, 0.86\n",
      "424.03644444444444, 38823.81333333333, 0.48422222222222217, 0.0, 147, 0.3333333333333333, 1201.9333333333334, 311992, 0.8066666666666666\n",
      "385.86133333333333, 34037.1, 0.44137777777777765, 0.0, 147, 0.3333333333333333, 1390.6666666666667, 215867, 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Iris_dataset(df.values[:,:4], df['species'])\n",
    "test_dataset = Iris_dataset(df.values[:,:4], df['species'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True, **test_kwargs)\n",
    "\n",
    "generation = GA(generation_length=100,population_size=150,initial_size=256,initial_depth=5, lossFn=nn.CrossEntropyLoss(), input_size=4, output_size=3, outputLayer=nn.Softmax(dim=1), device=device, train_dataloader=train_loader, test_dataloader=test_loader, optimizerFn=\"Adam\", epoch=100  )\n",
    "generation.auto_evolve()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:53:43.299842Z",
     "start_time": "2024-01-07T12:51:17.417968Z"
    }
   },
   "id": "fe457d527ece803a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual #0: {'genome': [['D', 28], ['D', 9], ['D', 78], ['D', 17], ['D', 26]], 'model': ConstructNet(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=28, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=28, out_features=9, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=9, out_features=78, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=78, out_features=17, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=17, out_features=26, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=26, out_features=3, bias=True)\n",
      "    (11): Softmax(dim=1)\n",
      "  )\n",
      "), 'id': 78, 'loss': 0.06797236998875936, 'accuracy': 0.7066666666666667, 'parameters': 3073, 'fitness': 1024.6666666666667}\n",
      "Individual #1: {'genome': [['D', 28], ['D', 9], ['D', 146], ['D', 17]], 'model': ConstructNet(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=28, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=28, out_features=9, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=9, out_features=146, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=146, out_features=17, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=17, out_features=3, bias=True)\n",
      "    (9): Softmax(dim=1)\n",
      "  )\n",
      "), 'id': 65, 'loss': 0.06967234929402669, 'accuracy': 0.76, 'parameters': 4414, 'fitness': 1109.6}\n",
      "Individual #2: {'genome': [['D', 28], ['D', 9], ['D', 17], ['D', 26]], 'model': ConstructNet(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=28, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=28, out_features=9, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=9, out_features=17, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=17, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=3, bias=True)\n",
      "    (9): Softmax(dim=1)\n",
      "  )\n",
      "), 'id': 105, 'loss': 0.07128274520238241, 'accuracy': 0.82, 'parameters': 1120, 'fitness': 1205.3999999999999}\n",
      "Individual #3: {'genome': [['D', 18], ['D', 96], ['D', 17], ['D', 26]], 'model': ConstructNet(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=96, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=96, out_features=17, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=17, out_features=26, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=26, out_features=3, bias=True)\n",
      "    (9): Softmax(dim=1)\n",
      "  )\n",
      "), 'id': 52, 'loss': 0.06488236705462137, 'accuracy': 0.8866666666666667, 'parameters': 4112, 'fitness': 1312.2666666666667}\n",
      "Individual #4: {'genome': [['D', 28], ['D', 75], ['D', 9], ['D', 17]], 'model': ConstructNet(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=28, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=28, out_features=75, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=75, out_features=9, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=9, out_features=17, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=17, out_features=3, bias=True)\n",
      "    (9): Softmax(dim=1)\n",
      "  )\n",
      "), 'id': 53, 'loss': 0.0692382287979126, 'accuracy': 0.9333333333333333, 'parameters': 3223, 'fitness': 1390.6666666666667}\n"
     ]
    }
   ],
   "source": [
    "generation.print_population(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:53:43.300728Z",
     "start_time": "2024-01-07T12:53:43.297711Z"
    }
   },
   "id": "c45994f84f6f0df7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:53:43.304907Z",
     "start_time": "2024-01-07T12:53:43.300797Z"
    }
   },
   "id": "3df8019cac33b657",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:53:43.305153Z",
     "start_time": "2024-01-07T12:53:43.302451Z"
    }
   },
   "id": "9640fcfa12294fcf",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
